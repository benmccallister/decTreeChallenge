---
title: "Decision Tree Challenge"
subtitle: "Feature Importance and Categorical Variable Encoding"
format:
  html: default
  pdf: default
execute:
  echo: false
  eval: true
---
```{python}
#| label: load-and-model-python
#| echo: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# Load data
sales_data = pd.read_csv("salesPriceData.csv")

# Prepare model data (treating zipCode as numerical)
model_vars = ['SalePrice', 'LotArea', 'YearBuilt', 'GrLivArea', 'FullBath', 
              'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd', 'GarageCars', 'zipCode']
model_data = sales_data[model_vars].dropna()

# Split data
X = model_data.drop('SalePrice', axis=1)
y = model_data['SalePrice']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)

# Build decision tree
tree_model = DecisionTreeRegressor(max_depth=3, 
                                  min_samples_split=20, 
                                  min_samples_leaf=10, 
                                  random_state=123)
tree_model.fit(X_train, y_train)

print(f"Model built with {tree_model.get_n_leaves()} terminal nodes")
```


```{python}
#| label: visualize-tree-python
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Visualize tree
plt.figure(figsize=(10, 6))
plot_tree(tree_model, 
          feature_names=X_train.columns,
          filled=True, 
          rounded=True,
          fontsize=10,
          max_depth=3)
plt.title("Decision Tree (zipCode as Numerical)")
plt.tight_layout()
plt.show()
```


```{python}
#| label: feature-importance-python
#| echo: false

# Extract and display feature importance
importance_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': tree_model.feature_importances_
}).sort_values('Importance', ascending=False)

importance_df['Importance_Percent'] = (importance_df['Importance'] * 100).round(2)

# Check zipCode ranking
zipcode_rank = importance_df[importance_df['Feature'] == 'zipCode'].index[0] + 1
zipcode_importance = importance_df[importance_df['Feature'] == 'zipCode']['Importance_Percent'].iloc[0]
```

```{python}
#| label: importance-plot-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance
plt.figure(figsize=(8, 5))
plt.barh(range(len(importance_df)), importance_df['Importance'], 
         color='steelblue', alpha=0.7)
plt.yticks(range(len(importance_df)), importance_df['Feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance (zipCode as Numerical)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```


```{python}
#| label: categorical-python
#| echo: false
#| include: false
# One-hot encode zipCode
import pandas as pd

# Create one-hot encoded zipCode
zipcode_encoded = pd.get_dummies(model_data['zipCode'], prefix='zipCode')
model_data_cat = pd.concat([model_data.drop('zipCode', axis=1), zipcode_encoded], axis=1)

# Split data
X_cat = model_data_cat.drop('SalePrice', axis=1)
y_cat = model_data_cat['SalePrice']
X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(X_cat, y_cat, test_size=0.2, random_state=123)

# Build decision tree with one-hot encoded zipCode
tree_model_cat = DecisionTreeRegressor(max_depth=3, 
                                      min_samples_split=20, 
                                      min_samples_leaf=10, 
                                      random_state=123)
tree_model_cat.fit(X_train_cat, y_train_cat)

# Feature importance with one-hot encoded zipCode
importance_cat_df = pd.DataFrame({
    'Feature': X_train_cat.columns,
    'Importance': tree_model_cat.feature_importances_
}).sort_values('Importance', ascending=False)

importance_cat_df['Importance_Percent'] = (importance_cat_df['Importance'] * 100).round(2)

# Check zipCode features
zipcode_features = [col for col in X_train_cat.columns if col.startswith('zipCode')]
zipcode_importance = importance_cat_df[importance_cat_df['Feature'].isin(zipcode_features)]['Importance'].sum()
total_importance = importance_cat_df['Importance'].sum()
zipcode_percent = (zipcode_importance / total_importance * 100).round(2)
```


```{python}
#| label: visualize-tree-cat-python
#| echo: false
#| fig-width: 10
#| fig-height: 6

# Visualize tree with one-hot encoded zipCode
plt.figure(figsize=(10, 6))
plot_tree(tree_model_cat, 
          feature_names=X_train_cat.columns,
          filled=True, 
          rounded=True,
          fontsize=8,
          max_depth=4)
plt.title("Decision Tree (zipCode One-Hot Encoded)")
plt.tight_layout()
plt.show()
```

```{python}
#| label: importance-plot-cat-python
#| echo: false
#| fig-width: 8
#| fig-height: 5

# Plot feature importance for categorical zipCode
plt.figure(figsize=(8, 5))
plt.barh(range(len(importance_cat_df)), importance_cat_df['Importance'], 
         color='darkgreen', alpha=0.7)
plt.yticks(range(len(importance_cat_df)), importance_cat_df['Feature'])
plt.xlabel('Importance Score')
plt.title('Feature Importance (zipCode One-Hot Encoded)')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()
```

## Discussion Questions for Challenge

1. **Numerical vs Categorical Encoding:** There are two modelsin Python written above. For each language, the models differ by how zip code is modelled, either as a numerical variable or as a categorical variable. Given what you know about zip codes and real estate prices, how should zip code be modelled, numerically or categorically?  Is zipcode and ordinal or non-ordinal variable?

Answer: Zip code should be modelled as a categorical variable.  Zip code is a non-ordinal variable. There is no 'ranking' or 'ordering' of zip codes. Instead, zip codes are classifications of areas that are labeled numerically. Though the number itself is not meaningful for house price prediction, the category represented by the number is meaningful due to the location of the zip code, which is why it should be modelled as a categorical variable.

2. **R vs Python Implementation Differences:** When modelling zip code as a categorical variable, the output tree and feature importance would differ quite significantly had you used R as opposed to Python. Investigate why this is the case.  What does R offer that Python does not? Which language would you say does a better job of modelling zip code as a categorical variable? Can you quote the documentation at [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html) suggesting a weakness in the Python implementation? If so, please provide a quote from the documentation.

Answer: R offers the ability to group zip codes into categories, giving the ability to combine zip codes into groups with similar characteristics that would affect home prices. This is not possible in Python with one-hot encoding. According to the site listed above "the scikit-learn implementation does not support categorical variables for now". In Python, each zip code is treated as a separate category, making the zip code look unimportant. By grouping zip codes into categories, we are able to see the importance of zip code on home prices in a way we couldn't by making each zip code a separate category. For this reason, R does a better job of modelling zip code as a categorical variable.





